# -*- coding: utf-8 -*-
"""CompleteLR_Evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NEvmc2KiEVjfXpmYHG6ULopbGY-0MKK7

# Importing necessary libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

""">> Loading the dataset"""

df2 = pd.read_csv('/content/Regression3.csv')

df2.head(10)

df2.shape

df2.info()

df2.describe()

df2.isnull().sum()

df2.duplicated().sum()

df2.columns

"""Linearity of Dataset

>> **X1 alone in the dataset is Negatively correlated,
While Rest are positively correlated**
"""

sns.regplot(x='X1',y='Y',data=df2,marker='o',color='red')
plt.show()

sns.regplot(x='X2',y='Y',data=df2,marker='o',color='orange')
plt.show()

sns.regplot(x='X3',y='Y',data=df2,marker='o',color='yellow')
plt.show()

sns.regplot(x='X4',y='Y',data=df2,marker='o',color='green')
plt.show()

sns.regplot(x='X4.1',y='Y',data=df2,marker='o',color='blue')
plt.show()

sns.regplot(x='X6',y='Y',data=df2,marker='o', color='indigo')
plt.show()

sns.regplot(x='X7',y='Y',data=df2,marker='o', color='violet')
plt.show()

"""**Data Preprocessing**

>> Changing the column name
"""

df.rename(columns={'X1':'x1','X2':'x2','X3':'x3','X4':'x4','X4.1':'x4.1','X5':'x5','X6':'x6','X7':'x7','Y':'y'},inplace=True)

df.head()

"""# Model Fitting"""

X = df.drop('y',axis=1)
y = df['y']

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=42)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

X_train.shape

y_train.shape

from sklearn.linear_model import LinearRegression
lr = LinearRegression()
lr.fit(X_train,y_train)

y_pred = lr.predict(X_test)

"""# Evaluation â€“ MSE, MAE, R 2"""

from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
print('R2 Score:',r2_score(y_test,y_pred))
print('Mean Squared Error:',mean_squared_error(y_test,y_pred))
print('Mean Absolute Error:',mean_absolute_error(y_test,y_pred))

"""# Accuracy low due to presence of meagre outliers."""

plt.figure(figsize=(12,12))
sns.boxplot(df)
plt.show()

"""Hyperparameter tuning"""

from sklearn.

y_test.shape

y_pred.shape

"""# Model's Predicted Result"""

x2 = [125,256,6000,256,16,128,198]
x2 = np.array(x2)
x2 = x2.reshape(1,-1)
x2 = sc.transform(x2)
print(lr.predict(x2))

plt.scatter(y_test, y_pred , color='maroon')
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title("Actual vs Predicted")
plt.show()

"""# Mean of Residuals

# Homoscedasticity

# Normality
"""

# a. Mean of Residuals
# b. Check for Homoscedasticity
# c. Check for Normality of error terms/residuals



# Calculate residuals
residuals = y_test - y_pred

print("Mean of Residuals:", np.mean(residuals))

# b. Check for Homoscedasticity
plt.scatter(y_pred, residuals)
plt.xlabel("Predicted Values")
plt.ylabel("Residuals")
plt.title("Residuals vs Predicted Values (Homoscedasticity Check)")
plt.axhline(y=0, color='r', linestyle='--')
plt.show()

# c. Check for Normality of error terms/residuals
sns.distplot(residuals, kde=True)
plt.xlabel("Residuals")
plt.title("Distribution of Residuals (Normality Check)")
plt.show()

"""# Autocorrelation of residuals"""

import statsmodels.api as sm

sm.graphics.tsa.plot_acf(residuals, lags=40)
plt.show()

"""# Perfect multicollinearity"""

plt.figure(figsize=(12,10))
sns.heatmap(df2.corr(),annot=True,cmap='GnBu',linecolor='white',linewidths=0.5,square=True)
plt.show()

"""# Best independent variable"""

df2.corr()

"""Solution: **The best independent variable here is X7 with collinearity of 0.97
the second best is x3 with 0.90**
"""

